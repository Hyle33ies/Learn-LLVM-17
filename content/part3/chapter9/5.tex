
Using the layered approach of ORC, it is very easy to build a JIT compiler customized for the requirements. There is no one-size-fits-all JIT compiler, and the first section of this chapter gave some examples. Let’s have a look at how to set up a JIT compiler from scratch.

The ORC API uses layers that are stacked together. The lowest level is the object-linking layer, represented by the llvm::orc::RTDyldObjectLinkingLayer class. It is responsible for linking in-memory objects and turning them into executable code. The memory required for this task is managed by an instance of the MemoryManager interface. There is a default implementation, but we can also use a custom version if we need.

Above the object-linking layer is the compile layer, which is responsible for creating an in-memory object file. The llvm::orc::IRCompileLayer class takes an IR module as input and compiles it to an object file. The IRCompileLayer class is a subclass of the IRLayer class, which is a generic class for layer implementations accepting LLVM IR.

Both of these layers already form the core of a JIT compiler: they add an LLVM IR module as input, which is compiled and linked in memory. To add extra functionality, we can incorporate more layers on top of both layers.

For example, the CompileOnDemandLayer class splits a module so that only the requested functions are compiled. This can be used to implement lazy compilation. Moreover, the CompileOnDemandLayer class is also a subclass of the IRLayer class. In a very generic way, the IRTransformLayer class, also a subclass of the IRLayer class, allows us to apply a transformation to the module.

Another important class is the ExecutionSession class. This class represents a running JIT program. Essentially, this means that the class manages JITDylib symbol tables, provides lookup functionality for symbols, and keeps track of used resource managers.

The generic recipe for a JIT compiler is as follows:

\begin{enumerate}
\item
Initialize an instance of the ExecutionSession class.

\item
Initialize the layer, at least consisting of an RTDyldObjectLinkingLayer class and an IRCompileLayer class.

\item
Create the first JITDylib symbol table, usually with main or a similar name.
\end{enumerate}

The general usage of the JIT compiler is also very straightforward:

\begin{enumerate}
\item
Add an IR module to the symbol table.

\item
Look up a symbol, triggering the compilation of the associated function, and possibly the whole module.

\item
Execute the function.
\end{enumerate}

In the next subsection, we implement a JIT compiler class following the generic recipe.

\mySubsubsection{9.5.1.}{Creating a JIT compiler class}

To keep the implementation of the JIT compiler class simple, everything is placed in JIT.h, within a source directory we can create called jit. However, the initialization of the class is a bit more complex compared to using LLJIT. Due to handling possible errors, we need a factory method to create some objects upfront before we can call the constructor. The steps to create the class are as follows:

\begin{enumerate}
\item
We begin with guarding the header file against multiple inclusion with the JIT\_H preprocessor definition:

\begin{cpp}
#ifndef JIT_H
#define JIT_H
\end{cpp}

\item
Firstly, a number of include files are required. Most of them provide a class with the same name as the header file. The Core.h header provides a couple of basic classes, including the ExecutionSession class. Additionally, the ExecutionUtils.h header provides the DynamicLibrarySearchGenerator class to search libraries for symbols. Furthermore, the CompileUtils.h header provides the ConcurrentIRCompiler class:

\begin{cpp}
#include "llvm/Analysis/AliasAnalysis.h"
#include "llvm/ExecutionEngine/JITSymbol.h"
#include "llvm/ExecutionEngine/Orc/CompileUtils.h"
#include "llvm/ExecutionEngine/Orc/Core.h"
#include "llvm/ExecutionEngine/Orc/ExecutionUtils.h"
#include "llvm/ExecutionEngine/Orc/IRCompileLayer.h"
#include "llvm/ExecutionEngine/Orc/IRTransformLayer.h"
#include "llvm/ExecutionEngine/Orc/JITTargetMachineBuilder.h"
#include "llvm/ExecutionEngine/Orc/Mangling.h"
#include "llvm/ExecutionEngine/Orc/RTDyldObjectLinkingLayer.h"
#include "llvm/ExecutionEngine/Orc/TargetProcessControl.h"
#include "llvm/ExecutionEngine/SectionMemoryManager.h"
#include "llvm/Passes/PassBuilder.h"
#include "llvm/Support/Error.h"
\end{cpp}

\item
Declare a new class. Our new class will be called JIT:

\begin{cpp}
class JIT {
\end{cpp}

\item
The private data members reflect the ORC layers and some helper classes. The ExecutionSession, ObjectLinkingLayer, CompileLayer, OptIRLayer, and MainJITDylib instances represent the running JIT program, the layers, and the symbol table, as already described. Moreover, the TargetProcessControl instance is used for interaction with the JIT target process. This can be the same process, another process on the same machine, or a remote process on a different machine, possibly with a different architecture. The DataLayout and MangleAndInterner classes are required to mangle symbols’ names in the correct way. Additionally, the symbol names are internalized, which means that all equal names have the same address. This means that to check if two symbol names are equal, it is then sufficient to compare the addresses, which is a very fast operation:

\begin{cpp}
    std::unique_ptr<llvm::orc::TargetProcessControl> TPC;
    std::unique_ptr<llvm::orc::ExecutionSession> ES;
    llvm::DataLayout DL;
    llvm::orc::MangleAndInterner Mangle;
    std::unique_ptr<llvm::orc::RTDyldObjectLinkingLayer>
        ObjectLinkingLayer;
    std::unique_ptr<llvm::orc::IRCompileLayer>
        CompileLayer;
    std::unique_ptr<llvm::orc::IRTransformLayer>
        OptIRLayer;
    llvm::orc::JITDylib &MainJITDylib;
\end{cpp}

\item
The initialization is split into three parts. In C++, a constructor cannot return an error. The simple and recommended solution is to create a static factory method that can do the error handling before constructing an object. The initialization of the layers is more complex, so we introduce factory methods for them, too.

In the create() factory method, we first create a SymbolStringPool instance, which is used to implement string internalization and is shared by several classes. To take control of the current process, we create a SelfTargetProcessControl instance. If we want to target a different process, then we need to change this instance.

Next, we construct a JITTargetMachineBuilder instance, for which we need to know the target triple of the JIT process. Afterward, we query the target machine builder for the data layout. This step can fail if the builder is not able to instantiate the target machine based on the provided triple – for example, because support for this target is not compiled into the LLVM libraries:

\begin{cpp}
public:
    static llvm::Expected<std::unique_ptr<JIT>> create() {
        auto SSP =
            std::make_shared<llvm::orc::SymbolStringPool>();
        auto TPC =
            llvm::orc::SelfTargetProcessControl::Create(SSP);
        if (!TPC)
            return TPC.takeError();
        llvm::orc::JITTargetMachineBuilder JTMB(
            (*TPC)->getTargetTriple());
        auto DL = JTMB.getDefaultDataLayoutForTarget();
        if (!DL)
            return DL.takeError();
\end{cpp}

\item
At this point, we have handled all calls that could potentially fail. We are now able to initialize the ExecutionSession instance. Finally, the constructor of the JIT class is called with all instantiated objects, and the result is returned to the caller:

\begin{cpp}
        auto ES =
            std::make_unique<llvm::orc::ExecutionSession>(
                std::move(SSP));

        return std::make_unique<JIT>(
            std::move(*TPC), std::move(ES), std::move(*DL),
            std::move(JTMB));
    }
\end{cpp}

\item
The constructor of the JIT class moves the passed parameters to the private data members. Layer objects are constructed with a call to static factory names with the create prefix. Furthermore, each layer factory method requires a reference to the ExecutionSession instance, which connects the layer to the running JIT session. Except for the object-linking layer, which is at the bottom of the layer stack, each layer requires a reference to the previous layer, illustrating the stacking order:

\begin{cpp}
JIT(std::unique_ptr<llvm::orc::ExecutorProcessControl>
            EPCtrl,
    std::unique_ptr<llvm::orc::ExecutionSession>
        ExeS,
    llvm::DataLayout DataL,
    llvm::orc::JITTargetMachineBuilder JTMB)
    : EPC(std::move(EPCtrl)), ES(std::move(ExeS)),
        DL(std::move(DataL)), Mangle(*ES, DL),
        ObjectLinkingLayer(std::move(
            createObjectLinkingLayer(*ES, JTMB))),
        CompileLayer(std::move(createCompileLayer(
            *ES, *ObjectLinkingLayer,
            std::move(JTMB)))),
        OptIRLayer(std::move(
            createOptIRLayer(*ES, *CompileLayer))),
        MainJITDylib(
            ES->createBareJITDylib("<main>")) {
\end{cpp}

\item
In the body of the constructor, we add a generator to search the current process for symbols. The GetForCurrentProcess() method is special, as the return value is wrapped in an Expected<> template, indicating that an Error object can also be returned. However, since we know that no error can occur, the current process will eventually run! Thus, we unwrap the result with the cantFail() function, which terminates the application if an error occurred anyway:

\begin{cpp}
    MainJITDylib.addGenerator(llvm::cantFail(
        llvm::orc::DynamicLibrarySearchGenerator::
            GetForCurrentProcess(DL.getGlobalPrefix())));
}
\end{cpp}

\item
To create an object-linking layer, we need to provide a memory manager. Here, we stick to the default SectionMemoryManager class, but we could also provide a different implementation if needed:

\begin{cpp}
    static std::unique_ptr<
        llvm::orc::RTDyldObjectLinkingLayer>
    createObjectLinkingLayer(
        llvm::orc::ExecutionSession &ES,
        llvm::orc::JITTargetMachineBuilder &JTMB) {
    auto GetMemoryManager = []() {
        return std::make_unique<
            llvm::SectionMemoryManager>();
    };
    auto OLLayer = std::make_unique<
        llvm::orc::RTDyldObjectLinkingLayer>(
        ES, GetMemoryManager);
\end{cpp}

\item
A slight complication exists for the Common Object File Format (COFF) object file format, which is used on Windows. This file format does not allow functions to be marked as exported. This subsequently leads to failures in checks inside the object-linking layer: flags stored in the symbol are compared with the flags from IR, which leads to a mismatch because of the missing export marker. The solution is to override the flags only for this file format. This finishes the construction of the object layer, and the object is returned to the caller:

\begin{cpp}
   if (JTMB.getTargetTriple().isOSBinFormatCOFF()) {
       OLLayer
            ->setOverrideObjectFlagsWithResponsibilityFlags(
                true);
       OLLayer
            ->setAutoClaimResponsibilityForObjectSymbols(
                true);
   }
   return OLLayer;
}
\end{cpp}

\item
To initialize the compiler layer, an IRCompiler instance is required. The IRCompiler instance is responsible for compiling an IR module into an object file. If our JIT compiler does not use threads, then we can use the SimpleCompiler class, which compiles the IR module using a given target machine. The TargetMachine class is not threadsafe, and therefore the SimpleCompiler class is not, either. To support compilation with multiple threads, we use the ConcurrentIRCompiler class, which creates a new TargetMachine instance for each module to compile. This approach solves the problem with multiple threads:

\begin{cpp}
    static std::unique_ptr<llvm::orc::IRCompileLayer>
    createCompileLayer(
            llvm::orc::ExecutionSession &ES,
            llvm::orc::RTDyldObjectLinkingLayer &OLLayer,
            llvm::orc::JITTargetMachineBuilder JTMB) {
        auto IRCompiler = std::make_unique<
            llvm::orc::ConcurrentIRCompiler>(
            std::move(JTMB));
        auto IRCLayer =
            std::make_unique<llvm::orc::IRCompileLayer>(
                ES, OLLayer, std::move(IRCompiler));
        return IRCLayer;
    }
\end{cpp}

\item
Instead of compiling the IR module directly to machine code, we install a layer that optimizes the IR first. This is a deliberate design decision: we turn our JIT compiler into an optimizing JIT compiler, which produces faster code that takes longer to produce, meaning a delay for the user. We do not add lazy compilation, so whole modules are compiled when just a symbol is looked up. This can add up to a significant amount of time before the user sees the code executing.

\begin{myNotic}{Note}
Introducing lazy compilation is not a proper solution in all circumstances. Lazy compilation is realized by moving each function into a module of its own, which is compiled when the function name is looked up. This prevents inter-procedural optimizations such as inlining because the inliner pass needs access to the body of called functions to inline them. As a result, users see a faster startup with lazy compilation, but the produced code is not as optimal as it can be. These design decisions depend on the intended use. Here, we decide on fast code, accepting a slower startup time. Furthermore, this means that the optimization layer is essentially a transformation layer.
\end{myNotic}

The IRTransformLayer class delegates the transformation to a function – in our case, to the optimizeModule function:

\begin{cpp}
    static std::unique_ptr<llvm::orc::IRTransformLayer>
    createOptIRLayer(
            llvm::orc::ExecutionSession &ES,
            llvm::orc::IRCompileLayer &CompileLayer) {
        auto OptIRLayer =
        std::make_unique<llvm::orc::IRTransformLayer>(
            ES, CompileLayer,
            optimizeModule);
        return OptIRLayer;
    }
\end{cpp}

\item
The optimizeModule() function is an example of a transformation on an IR module. The function gets the module to transform as a parameter and returns the transformed version of the IR module. Since the JIT compiler can potentially run with multiple threads, the IR module is wrapped in a ThreadSafeModule instance:

\begin{cpp}
    static llvm::Expected<llvm::orc::ThreadSafeModule>
    optimizeModule(
        llvm::orc::ThreadSafeModule TSM,
        const llvm::orc::MaterializationResponsibility
        &R) {
\end{cpp}

\item
To optimize the IR, we recall some information from Chapter 7, Optimizing IR, in the Adding an optimization pipeline to your compiler section. We need a PassBuilder instance to create an optimization pipeline. First, we define a couple of analysis managers and register them afterward at the pass builder. Afterward, we populate a ModulePassManager instance with the default optimization pipeline for the O2 level. This is again a design decision: the O2 level produces already fast machine code, but it produces even faster code at the O3 level. Next, we run the pipeline on the module, and finally, the optimized module is returned to the caller:

\begin{cpp}
    TSM.withModuleDo([](llvm::Module &M) {
        bool DebugPM = false;
        llvm::PassBuilder PB(DebugPM);
        llvm::LoopAnalysisManager LAM(DebugPM);
        llvm::FunctionAnalysisManager FAM(DebugPM);
        llvm::CGSCCAnalysisManager CGAM(DebugPM);
        llvm::ModuleAnalysisManager MAM(DebugPM);
        FAM.registerPass(
            [&] { return PB.buildDefaultAAPipeline(); });
        PB.registerModuleAnalyses(MAM);
        PB.registerCGSCCAnalyses(CGAM);
        PB.registerFunctionAnalyses(FAM);
        PB.registerLoopAnalyses(LAM);
        PB.crossRegisterProxies(LAM, FAM, CGAM, MAM);
        llvm::ModulePassManager MPM =
            PB.buildPerModuleDefaultPipeline(
                llvm::PassBuilder::OptimizationLevel::O2,
                DebugPM);
        MPM.run(M, MAM);
    });

    return TSM;
}
\end{cpp}

\item
The client of the JIT class needs a way to add an IR module, which we provide with the addIRModule() function. Recall the layer stack we created: we must add the IR module to the top layer; otherwise, we would accidentally bypass some of the layers. This would be a programming error that is not easily spotted: if the OptIRLayer member is replaced by the CompileLayer member, then our JIT class still works, but not as an optimizing JIT because we have bypassed this layer. This is no concern for this small implementation, but in a large JIT optimization, we would introduce a function to return the top-level layer:

\begin{cpp}
    llvm::Error addIRModule(
            llvm::orc::ThreadSafeModule TSM,
            llvm::orc::ResourceTrackerSP RT = nullptr) {
        if (!RT)
            RT = MainJITDylib.getDefaultResourceTracker();
        return OptIRLayer->add(RT, std::move(TSM));
    }
\end{cpp}

\item
Likewise, a client of our JIT class needs a way to look up a symbol. We delegate this to the ExecutionSession instance, passing in a reference to the main symbol table and the mangled and internalized name of the requested symbol:

\begin{cpp}
    llvm::Expected<llvm::orc::ExecutorSymbolDef>
    lookup(llvm::StringRef Name) {
        return ES->lookup({&MainJITDylib},
                            Mangle(Name.str()));
    }
\end{cpp}
\end{enumerate}

As we can see, the initialization of this JIT class can be tricky, as it involves a factory method and a constructor call for the JIT class, and factory methods for each layer. Although this distribution is caused by limitations in C++, the code itself is straightforward.

Next, we are going to use the new JIT compiler class to implement a simple command-line utility that takes an LLVM IR file as input.

\mySubsubsection{9.5.2.}{Using our new JIT compiler class}

We start off by creating a file called JIT.cpp, in the same directory as the JIT.h file, and add the following to this source file:

\begin{enumerate}
\item
Firstly, several header files are included. We must include JIT.h to use our new class, and the IRReader.h header because it defines a function to read LLVM IR files. The CommandLine.h header allows us to parse the command-line options in the LLVM style. Next, InitLLVM.h is needed for the basic initialization of the tool. Finally, TargetSelect.h is needed for the initialization of the native target:

\begin{cpp}
#include "JIT.h"
#include "llvm/IRReader/IRReader.h"
#include "llvm/Support/CommandLine.h"
#include "llvm/Support/InitLLVM.h"
#include "llvm/Support/TargetSelect.h"
\end{cpp}

\item
Next, we add the llvm namespace to the current scope:

\begin{cpp}
using namespace llvm;
\end{cpp}

\item
Our JIT tool expects exactly one input file on the command line, which we declare with the cl::opt<> class:

\begin{cpp}
static cl::opt<std::string>
    InputFile(cl::Positional, cl::Required,
        cl::desc("<input-file>"));
\end{cpp}

\item
To read the IR file, we call the parseIRFile() function. The file can be a textual IR representation or a bitcode file. The function returns a pointer to the created module. Additionally, the error handling is a bit different, because a textual IR file can be parsed, which is not necessarily syntactically correct. Finally, the SMDiagnostic instance holds the error information in case of a syntax error. In the event of an error, an error message is printed, and the application is exited:

\begin{cpp}
std::unique_ptr<Module>
loadModule(StringRef Filename, LLVMContext &Ctx,
            const char *ProgName) {
    SMDiagnostic Err;
    std::unique_ptr<Module> Mod =
        parseIRFile(Filename, Err, Ctx);
    if (!Mod.get()) {
        Err.print(ProgName, errs());
        exit(-1);
    }
    return Mod;
}
\end{cpp}

\item
The jitmain() function is placed after the loadModule() method. This function sets up our JIT engine and compiles an LLVM IR module. The function needs the LLVM module with the IR to execute. The LLVM context class is also required for this module because the context class contains important type information. The goal is to call the main() function, so we also pass the usual argc and argv parameters:

\begin{cpp}
Error jitmain(std::unique_ptr<Module> M,
    std::unique_ptr<LLVMContext> Ctx,
    int argc, char *argv[]) {
\end{cpp}

\item
Next, we create an instance of our JIT class that we constructed earlier. If an error occurs, then we return an error message accordingly:

\begin{cpp}
auto JIT = JIT::create();
if (!JIT)
    return JIT.takeError();
\end{cpp}

\item
Then, we add the module to the main JITDylib instance, wrapping the module and a context in a ThreadSafeModule instance yet again. If an error occurs, then we return an error message:

\begin{cpp}
if (auto Err = (*JIT)->addIRModule(
        orc::ThreadSafeModule(std::move(M),
                              std::move(Ctx))))
    return Err;
\end{cpp}

\item
Following this, we look up the main symbol. This symbol must be in the IR module given on the command line. The lookup triggers the compilation of that IR module. If other symbols are referenced inside the IR module, then they are resolved using the generator added in the previous step. The result is of the ExecutorAddr class, where it represents the address of the executor process:

\begin{cpp}
    llvm::orc::ExecutorAddr MainExecutorAddr = MainSym->getAddress();
    auto *Main = MainExecutorAddr.toPtr<int(int, char**)>();
\end{cpp}

\item
Now, we can call the main() function in the IR module, and pass the argc and argv parameters that the function expects. We ignore the return value:

\begin{cpp}
    (void)Main(argc, argv);
\end{cpp}

\item
We report success after the execution of the function:

\begin{cpp}
   return Error::success();
}
\end{cpp}

\item
After implementing a jitmain() function, we add a main() function, which initializes the tool and the native target and parses the command line:

\begin{cpp}
int main(int argc, char *argv[]) {
    InitLLVM X(argc, argv);
    InitializeNativeTarget();
    InitializeNativeTargetAsmPrinter();
    InitializeNativeTargetAsmParser();

    cl::ParseCommandLineOptions(argc, argv, "JIT\n");
\end{cpp}

\item
Afterward, the LLVM context class is initialized, and we load the IR module named on the command line:

\begin{cpp}
    auto Ctx = std::make_unique<LLVMContext>();
    std::unique_ptr<Module> M =
        loadModule(InputFile, *Ctx, argv[0]);
\end{cpp}

\item
After loading the IR module, we can call the jitmain() function. To handle errors, we use the ExitOnError utility class to print an error message and exit the application when an error is encountered. We also set a banner with the name of the application, which is printed before the error message:

\begin{cpp}
    ExitOnError ExitOnErr(std::string(argv[0]) + ": ");
    ExitOnErr(jitmain(std::move(M), std::move(Ctx),
                      argc, argv));
\end{cpp}

\item
If the control flow reaches this point, then the IR was successfully executed. We return 0 to indicate success:

\begin{cpp}
    return 0;
}
\end{cpp}
\end{enumerate}

We can now test our newly implemented JIT compiler by compiling a simple example that prints Hello World! to the console. Under the hood, the new class uses a fixed optimization level, so with large enough modules, we can note differences in the startup and runtime.

To build our JIT compiler, we can follow the same CMake steps as we did near the end of the Implementing our own JIT compiler with LLJIT section, and we just need to ensure that the JIT.cpp source file is being compiled with the correct libraries to link against:

\begin{cmake}
add_executable(JIT JIT.cpp)
include_directories(${CMAKE_SOURCE_DIR})
target_link_libraries(JIT ${llvm_libs})
\end{cmake}

We then change into the build directory and compile the application:

\begin{shell}
$ cmake –G Ninja <path to jit source directory>
$ ninja
\end{shell}

Our JIT tool is now ready to be used. A simple Hello World! program can be written in C, like the following:

\begin{shell}
$ cat main.c
#include <stdio.h>

int main(int argc, char** argv) {
    printf("Hello world!\n");
    return 0;
}
\end{shell}

Next, we can compile the Hello World C source into LLVM IR with the following command:

\begin{shell}
$ clang -S -emit-llvm main.c
\end{shell}

Remember – we compile the C source into LLVM IR because our JIT compiler accepts an IR file as input. Finally, we can invoke our JIT compiler with our IR example, as follows:

\begin{shell}
$ JIT main.ll
Hello world!
\end{shell}










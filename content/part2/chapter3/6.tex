As shown in the previous chapter, the parser is derived from the grammar. Let’s recall all the construction rules. For each rule of the grammar, you create a method named after the non-terminal on the lefthand side of the rule to parse the right-hand side of the rule. Following the definition of the right-hand side, you do the following:

\begin{itemize}
\item
For each non-terminal, the corresponding method is called

\item
Each token is consumed

\item
For alternatives and optional or repeating groups, the look-ahead token (the next unconsumed token) is examined to decide where to continue
\end{itemize}

Let’s apply these construction rules to the following rule of grammar:

\begin{shell}
ifStatement
    : "IF" expression "THEN" statementSequence
        ( "ELSE" statementSequence )? "END" ;
\end{shell}

We can easily translate this into the following C++ method:

\begin{cpp}
void Parser::parseIfStatement() {
    consume(tok::kw_IF);
    parseExpression();
    consume(tok::kw_THEN);
    parseStatementSequence();
    if (Tok.is(tok::kw_ELSE)) {
        advance();
        parseStatementSequence();
    }
    consume(tok::kw_END);
}
\end{cpp}

The whole grammar of tinylang can be turned into C++ in this way. In general, you have to be careful to avoid some pitfalls since most grammars that you will find on the internet are not suitable for this kind of construction.

\begin{myTip}{Grammars and parsers}
There are two different types of parsers: top-down parsers and bottom-up parsers. Their names are derived from the order in which a rule is handled during parsing. The input for a parser is the sequence of tokens generated by the lexer.
A top-down parser expands the leftmost symbol in a rule until a token is matched. Parsing is successful if all tokens are consumed and all symbols are expanded. This is exactly how the parser for tinylang works.

A bottom-up parser does the opposite: it looks at the sequence of tokens and tries to replace the tokens with a symbol of the grammar. For example, if the next tokens are IF, 3, +, and 4, then a bottom-up parser replaces the 3 + 4 token with the expression symbol, resulting in the IF expression sequence. When all tokens that belong to the IF statement are seen, then this sequence of tokens and symbols is replaced by the ifStatement symbol.

The parsing is successful if all tokens are consumed and the only symbol left is the start symbol. While top-down parsers can easily be constructed by hand, this is not the case for bottom-up parsers.

A different way to characterize both types of parsers is by which symbols are expanded first. Both read the input from left to right, but a top-down parser expands the leftmost symbol first while a bottom-up parser expands the rightmost symbol. Because of this, a top-down parser is also called an LL parser, while a bottom-up parser is called an LR parser.

A grammar must have certain properties so that either an LL or an LR parser can be derived from it. The grammars are named accordingly: you need an LL grammar to construct an LL parser.

You can find more details in university textbooks about compiler construction, such as Wilhelm, Seidl, and Hack: Compiler Design. Syntactic and Semantic Analysis, Springer 2013, and Grune and Jacobs: Parsing Techniques, A practical guide, Springer 2008.
\end{myTip}

One issue to look for is left-recursive rules. A rule is called left-recursive if the right-hand side begins with the same terminal that’s on the left-hand side. A typical example can be found in grammars for expressions:

\begin{shell}
expression : expression "+" term ;
\end{shell}

If it’s not already clear from the grammar, then the translation to C++ makes it obvious that this results in infinite recursion:

\begin{cpp}
void Parser::parseExpression() {
    parseExpression();
    consume(tok::plus);
    parseTerm();
}
\end{cpp}

Left recursion can also occur indirectly and involve more rules, which is much more difficult to spot. That’s why an algorithm exists that can detect and eliminate left recursion.

\begin{myNotic}{Note}
Left-recursive rules are only a problem for LL parsers, such as the recursive-descent parser for tinylang. The reason is that these parsers expand the leftmost symbol first. In contrast, if you use a parser generator to generate an LR parser, which expands the rightmost symbol first, then you should avoid right-recursive rules.
\end{myNotic}

At each step, the parser decides how to continue by just using the look-ahead token. The grammar is said to have conflicts if this decision cannot be made deterministically. To illustrate this, have a look at the using statement in C\#. Like in C++, the using statement can be used to make a symbol visible in a namespace, such as in using Math;. It is also possible to define an alias name for the imported symbol with using M = Math;. In a grammar, this can be expressed as follows:

\begin{shell}
usingStmt : "using" (ident "=")? ident ";"
\end{shell}

There’s a problem here: after the parser consumes the using keyword, the look-ahead token is ident. However, this information is not enough for us to decide if the optional group must be skipped or parsed. This situation always arises if the set of tokens, with which the optional group can begin, overlaps with the set of tokens that follow the optional group.

Let’s rewrite the rule with an alternative instead of an optional group:

\begin{shell}
usingStmt : "using" ( ident "=" ident | ident ) ";" ;
\end{shell}

Now, there is a different conflict: both alternatives begin with the same token. Looking only at the look-ahead token, the parser can’t decide which of the alternatives is the right one.

These conflicts are very common. Therefore, it’s good to know how to handle them. One approach is to rewrite the grammar in such a way that the conflict disappears. In the previous example, both alternatives begin with the same token. This can be factored out, resulting in the following rule:

\begin{shell}
usingStmt : "using" ident ("=" ident)? ";" ;
\end{shell}

This formulation has no conflict, but it should also be noted that it is less expressive. In the two other formulations, it is obvious which ident is the alias name and which ident is the namespace name. In the conflict-free rule, the leftmost ident changes its role. First, it is the namespace name, but if an equals sign follows, then it turns into the alias name.

The second approach is to add a predicate to distinguish between both cases. This predicate, often called a resolver, could use context information for the decision (such as a name lookup in a symbol table) or it could have a look at more than one token. Let’s assume that the lexer has a method called Token \&peek(int n) that returns the nth token after the current look-ahead token. Here, the existence of an equals sign can be used as an additional predicate in the decision:


\begin{cpp}
if (Tok.is(tok::ident) && Lex.peek(0).is(tok::equal)) {
    advance();
    consume(tok::equal);
}
consume(tok::ident);
\end{cpp}

A third approach is to use backtracking. For this, you need to save the current state. Then, you must try to parse the conflicting group. If this does not succeed, then you need to go back to the saved state and try the other path. Here, you are searching for the correct rule to apply, which is not as efficient as the other methods. Therefore, you should only use this approach as a last resort.

Now, let’s incorporate error recovery. In the previous chapter, I introduced the so-called panic mode as a technique for error recovery. The basic idea is to skip tokens until one is found that is suitable for continuing parsing. For example, in tinylang, a statement is followed by a semicolon (:).

If there is a syntax problem in an IF statement, then you skip all tokens until you find a semicolon. Then, you continue with the next statement. Instead of using an ad hoc definition for the token set, it’s better to use a systematic approach.

For each non-terminal, you compute the set of tokens that can follow the non-terminal anywhere (called the FOLLOW set). For the non-terminal statement, the ;, ELSE, and END tokens can follow. So, you must use this set in the error recovery part of parseStatement(). This method assumes that a syntax error can be handled locally. In general, this is not possible. Because the parser skips tokens, so many could be skipped that the end of input is reached. At this point, local recovery is not possible.

To prevent meaningless error messages, the calling method needs to be informed that an error recovery is still not finished. This can be done with bool. If it returns true, this means that error recovery hasn’t finished yet, while false means that parsing (including a possible error recovery) was successful.

There are numerous ways to extend this error recovery scheme. Using the FOLLOW sets of active callers is a popular approach. As a simple example, assume that parseStatement() was called by parseStatementSequence(), which was itself called by parseBlock() and that from parseModule().

Here, each of the corresponding non-terminals has a FOLLOW set. If the parser detects a syntax error in parseStatement(), then tokens are skipped until the token is in at least one of the FOLLOW sets of the active callers. If the token is in the FOLLOW set of the statement, then the error was recovered locally and a false value is returned to the caller. Otherwise, a true value is returned, meaning that error recovery must continue. A possible implementation strategy for this extension is passing std::bitset or std::tuple to represent the union of the current FOLLOW sets to the callee.

One last question is still open: how can we call the error recovery? In the previous chapter, goto was used to jump to the error recovery block. This works but is not a pleasing solution. Given what we discussed earlier, we can skip tokens in a separate method. Clang has a method has skipUntil() for this purpose; we also use this for tinylang.

Because the next step is to add semantic actions to the parser, it would also be nice to have a central place to put cleanup code if necessary. A nested function would be ideal for this. C++ does not have a nested function. Instead, a Lambda function can serve a similar purpose. The parseIfStatement() method, which we looked at initially, looks as follows when the complete error recovery code is added:

\begin{cpp}
bool Parser::parseIfStatement() {
    auto _errorhandler = [this] {
        return skipUntil(tok::semi, tok::kw_ELSE, tok::kw_END);
    };
    if (consume(tok::kw_IF))
        return _errorhandler();
    if (parseExpression(E))
        return _errorhandler();
    if (consume(tok::kw_THEN))
        return _errorhandler();
    if (parseStatementSequence(IfStmts))
        return _errorhandler();
    if (Tok.is(tok::kw_ELSE)) {
        advance();
        if (parseStatementSequence(ElseStmts))
        return _errorhandler();
    }
    if (expect(tok::kw_END))
        return _errorhandler();
    return false;
}
\end{cpp}

\begin{myTip}{Parser and lexer generators}
Manually constructing a parser and a lexer can be a tedious task, especially if you try to invent a new programming language and change the grammar very often. Luckily, some tools automate this task.

The classic Linux tools are flex (\url{https://github.com/westes/flex}) and bison (\url{https://www.gnu.org/software/bison/}). flex generates a lexer from a set of regular expressions, while bison generates an LALR(1) parser from a grammar description.
Both tools generate C/C+ source code and can be used together.

Another popular tool is AntLR (\url{https://www.antlr.org/}). AntLR can generate a lexer, a parser, and an AST from a grammar description. The generated parser belongs to the LL(*) class, which means it is a top-down parser that uses a variable number of lookaheads to solve conflicts. The tool is written in Java but can generate source code for many popular languages, including C/C++.

All these tools require some library support. If you are looking for a tool that generates a selfcontained lexer and parser, then Coco/R (\url{https://ssw.jku.at/Research/Projects/Coco/}) may be the tool for you. Coco/R generates a lexer and a recursive-descent parser from an LL(1) grammar description, similar to the one used in this book. The generated files are based on a template file that you can change if needed. The tool is written in C\# but ports to C++, Java, and other languages.

There are many other tools available, and they vary a lot in terms of the features and output languages they support. Of course, when choosing a tool, there are also trade-offs to consider. An LALR(1) parser generator such as bison can consume a wide range of grammars, and free grammars you can find on the internet are often LALR(1) grammars.

As a downside, these generators generate a state machine that needs to be interpreted at runtime, which can be slower than a recursive descent parser. Error handling is also more complicated. bison has basic support for handling syntax errors, but the correct use requires a deep understanding of how the parser works. Compared to this, AntLR consumes a slightly smaller grammar class but automatically generates error handling, and can also generate an AST. So, rewriting grammar so that it can be used with AntLR may speed up development later.
\end{myTip}
















































